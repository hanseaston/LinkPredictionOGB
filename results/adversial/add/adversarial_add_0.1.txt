==> Training graphsage: adversial add 0.1
=> Using cuda
Epoch 1: loss: 1.3453
Epoch 2: loss: 0.95364
=> max val = 0.014825191588820053
Epoch 3: loss: 0.83572
=> max val = 0.049704470031238526
Epoch 4: loss: 0.78751
Epoch 5: loss: 0.75369
Epoch 6: loss: 0.69985
=> max val = 0.08686108967780116
Epoch 7: loss: 0.65128
Epoch 8: loss: 0.6198
Epoch 9: loss: 0.59913
Epoch 10: loss: 0.58305
{'Hits@20': 0.0004195102218160298}
Epoch 11: loss: 0.5708
Epoch 12: loss: 0.55946
Epoch 13: loss: 0.54819
Epoch 14: loss: 0.53863
Epoch 15: loss: 0.52742
Epoch 16: loss: 0.51795
Epoch 17: loss: 0.50945
Epoch 18: loss: 0.49909
Epoch 19: loss: 0.4906
Epoch 20: loss: 0.48299
{'Hits@20': 0.0}
Epoch 21: loss: 0.47619
Epoch 22: loss: 0.46797
Epoch 23: loss: 0.46143
Epoch 24: loss: 0.45488
Epoch 25: loss: 0.45035
Epoch 26: loss: 0.44656
Epoch 27: loss: 0.44086
Epoch 28: loss: 0.43723
Epoch 29: loss: 0.4329
Epoch 30: loss: 0.4281
{'Hits@20': 0.0}
Epoch 31: loss: 0.42267
Epoch 32: loss: 0.41984
Epoch 33: loss: 0.41726
Epoch 34: loss: 0.41253
Epoch 35: loss: 0.41122
Epoch 36: loss: 0.40576
Epoch 37: loss: 0.40303
Epoch 38: loss: 0.40059
Epoch 39: loss: 0.39729
Epoch 40: loss: 0.39392
{'Hits@20': 0.0}
Epoch 41: loss: 0.38987
Epoch 42: loss: 0.38607
Epoch 43: loss: 0.38533
Epoch 44: loss: 0.38232
Epoch 45: loss: 0.37777
Epoch 46: loss: 0.37562
Epoch 47: loss: 0.37093
Epoch 48: loss: 0.36991
Epoch 49: loss: 0.36507
Epoch 50: loss: 0.36395
{'Hits@20': 0.00027717639655701966}
Epoch 51: loss: 0.36092
Epoch 52: loss: 0.35871
Epoch 53: loss: 0.35751
Epoch 54: loss: 0.35523
Epoch 55: loss: 0.35342
Epoch 56: loss: 0.35058
Epoch 57: loss: 0.34747
Epoch 58: loss: 0.34474
Epoch 59: loss: 0.34336
Epoch 60: loss: 0.34295
{'Hits@20': 0.0}
Epoch 61: loss: 0.33967
Epoch 62: loss: 0.33789
Epoch 63: loss: 0.33708
Epoch 64: loss: 0.33579
Epoch 65: loss: 0.33425
Epoch 66: loss: 0.3334
Epoch 67: loss: 0.3306
Epoch 68: loss: 0.32942
Epoch 69: loss: 0.3277
Epoch 70: loss: 0.32702
{'Hits@20': 2.9965015844002127e-05}
Epoch 71: loss: 0.3255
Epoch 72: loss: 0.32314
Epoch 73: loss: 0.32198
Epoch 74: loss: 0.3206
Epoch 75: loss: 0.31907
Epoch 76: loss: 0.3186
Epoch 77: loss: 0.31789
Epoch 78: loss: 0.31509
Epoch 79: loss: 0.31488
Epoch 80: loss: 0.31342
{'Hits@20': 7.491253961000532e-05}
Epoch 81: loss: 0.31443
Epoch 82: loss: 0.3123
Epoch 83: loss: 0.31125
Epoch 84: loss: 0.31091
Epoch 85: loss: 0.31137
Epoch 86: loss: 0.30871
Epoch 87: loss: 0.30767
Epoch 88: loss: 0.30799
Epoch 89: loss: 0.30826
Epoch 90: loss: 0.30608
{'Hits@20': 7.491253961000532e-06}
Epoch 91: loss: 0.30598
Epoch 92: loss: 0.30311
Epoch 93: loss: 0.30294
Epoch 94: loss: 0.30276
Epoch 95: loss: 0.30181
Epoch 96: loss: 0.30186
Epoch 97: loss: 0.30103
Epoch 98: loss: 0.29959
Epoch 99: loss: 0.29962
Epoch 100: loss: 0.29833
{'Hits@20': 1.4982507922001064e-05}
Epoch 101: loss: 0.29796
Epoch 102: loss: 0.29762
Epoch 103: loss: 0.2961
Epoch 104: loss: 0.29631
Epoch 105: loss: 0.29482
Epoch 106: loss: 0.29565
Epoch 107: loss: 0.29509
Epoch 108: loss: 0.29378
Epoch 109: loss: 0.2943
Epoch 110: loss: 0.29383
{'Hits@20': 0.0001872813490250133}
Epoch 111: loss: 0.29389
Epoch 112: loss: 0.29344
Epoch 113: loss: 0.29113
Epoch 114: loss: 0.29044
Epoch 115: loss: 0.29073
Epoch 116: loss: 0.29217
Epoch 117: loss: 0.28974
Epoch 118: loss: 0.28965
Epoch 119: loss: 0.29008
Epoch 120: loss: 0.28856
{'Hits@20': 7.491253961000532e-06}
Epoch 121: loss: 0.28948
Epoch 122: loss: 0.28735
Epoch 123: loss: 0.28729
Epoch 124: loss: 0.28548
Epoch 125: loss: 0.2871
Epoch 126: loss: 0.28538
Epoch 127: loss: 0.28663
Epoch 128: loss: 0.28581
Epoch 129: loss: 0.28582
Epoch 130: loss: 0.28504
{'Hits@20': 0.00014982507922001065}
Epoch 131: loss: 0.28332
Epoch 132: loss: 0.28306
Epoch 133: loss: 0.28497
Epoch 134: loss: 0.2829
Epoch 135: loss: 0.28265
Epoch 136: loss: 0.28115
Epoch 137: loss: 0.28143
Epoch 138: loss: 0.28112
Epoch 139: loss: 0.28204
Epoch 140: loss: 0.28072
{'Hits@20': 7.491253961000532e-06}
Epoch 141: loss: 0.2805
Epoch 142: loss: 0.28053
Epoch 143: loss: 0.27923
Epoch 144: loss: 0.28086
Epoch 145: loss: 0.28172
Epoch 146: loss: 0.28122
Epoch 147: loss: 0.27928
Epoch 148: loss: 0.27962
Epoch 149: loss: 0.27843
Epoch 150: loss: 0.27929
{'Hits@20': 2.9965015844002127e-05}
Epoch 151: loss: 0.27758
Epoch 152: loss: 0.281
Epoch 153: loss: 0.27785
Epoch 154: loss: 0.2775
Epoch 155: loss: 0.2771
Epoch 156: loss: 0.27475
Epoch 157: loss: 0.27487
Epoch 158: loss: 0.27568
Epoch 159: loss: 0.27613
Epoch 160: loss: 0.27402
{'Hits@20': 1.4982507922001064e-05}
Epoch 161: loss: 0.27532
Epoch 162: loss: 0.27602
Epoch 163: loss: 0.27326
Epoch 164: loss: 0.27393
Epoch 165: loss: 0.2739
Epoch 166: loss: 0.27447
Epoch 167: loss: 0.27377
Epoch 168: loss: 0.273
Epoch 169: loss: 0.27391
Epoch 170: loss: 0.27307
{'Hits@20': 0.00013484257129800959}
Epoch 171: loss: 0.27345
Epoch 172: loss: 0.27219
Epoch 173: loss: 0.27227
Epoch 174: loss: 0.27186
Epoch 175: loss: 0.27167
Epoch 176: loss: 0.27104
Epoch 177: loss: 0.27188
Epoch 178: loss: 0.27083
Epoch 179: loss: 0.27159
Epoch 180: loss: 0.27048
{'Hits@20': 7.491253961000532e-05}
Epoch 181: loss: 0.27019
Epoch 182: loss: 0.27126
Epoch 183: loss: 0.27191
Epoch 184: loss: 0.27017
Epoch 185: loss: 0.26889
Epoch 186: loss: 0.26909
Epoch 187: loss: 0.26924
Epoch 188: loss: 0.26832
Epoch 189: loss: 0.26862
Epoch 190: loss: 0.26869
{'Hits@20': 0.0}
Epoch 191: loss: 0.26823
Epoch 192: loss: 0.26863
Epoch 193: loss: 0.26914
Epoch 194: loss: 0.26825
Epoch 195: loss: 0.26858
Epoch 196: loss: 0.26707
Epoch 197: loss: 0.26725
Epoch 198: loss: 0.26876
Epoch 199: loss: 0.26649
Epoch 200: loss: 0.26798
{'Hits@20': 7.491253961000532e-06}
=> results/adversial/add/0.1/gnn_trained
	Best model: ep109_gnn.pt
==> Testing
	Edges scored
	Val scoring evaluated
{'Hits@20': 0.0006517390946070463, 'Hits@50': 0.0016106196016151143, 'Hits@100': 0.0036707144408902608}
{'Hits@20': 0.0, 'Hits@50': 2.9965015844002127e-05, 'Hits@100': 4.494752376600319e-05}
